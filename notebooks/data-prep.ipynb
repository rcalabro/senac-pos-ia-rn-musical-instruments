{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc38a96",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87a106",
   "metadata": {},
   "source": [
    "### Download Kaggle dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "dataset_path = '../datasets/raw'\n",
    "force_download = False\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\", force_download=force_download)\n",
    "print(\"Kaggle downloaded files:\", path)\n",
    "\n",
    "# Cria a pasta de destino, se n√£o existir\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "shutil.copytree(\n",
    "    src=path,\n",
    "    dst=dataset_path,\n",
    "    dirs_exist_ok=True  # Permite copiar para pasta existente\n",
    ")\n",
    "\n",
    "print(\"Moved kaggle files to:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f78511",
   "metadata": {},
   "source": [
    "### Organize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6331820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DATASET_PATH = '../datasets/raw'\n",
    "NORMALIZED_DATASET_PATH = '../datasets/normalized'\n",
    "AUGMENTED_DATASET_PATH = '../datasets/augmented'\n",
    "PREPARED_DATASET_PATH = '../datasets/prepared'\n",
    "\n",
    "os.makedirs(NORMALIZED_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "# Fun√ß√£o para normalizar nomes dos arquivos\n",
    "def normalize_filename(filename: str) -> str:\n",
    "    stem = Path(filename).stem\n",
    "    stem = re.sub(r\"[^a-zA-Z0-9]\", \"_\", stem).lower()\n",
    "    stem = re.sub(r\"_+\", \"_\", stem).strip(\"_\")\n",
    "    return f\"{stem}.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos RAW\n",
    "RAW_TRAIN_CSV_PATH = Path(os.path.join(RAW_DATASET_PATH, 'Metadata_Train.csv'))\n",
    "print(f\"RAW_TRAIN_CSV_PATH: {RAW_TRAIN_CSV_PATH}\")\n",
    "\n",
    "RAW_TRAIN_AUDIOS_DIR = Path(os.path.join(RAW_DATASET_PATH, 'Train_submission', 'Train_submission'))\n",
    "print(f\"RAW_TRAIN_AUDIOS_DIR: {RAW_TRAIN_AUDIOS_DIR}\")\n",
    "\n",
    "RAW_TEST_CSV_PATH = Path(os.path.join(RAW_DATASET_PATH, 'Metadata_Test.csv'))\n",
    "print(f\"RAW_TEST_CSV_PATH: {RAW_TEST_CSV_PATH}\")\n",
    "\n",
    "RAW_TEST_AUDIOS_DIR = Path(os.path.join(RAW_DATASET_PATH, 'Test_submission', 'Test_submission'))\n",
    "print(f\"RAW_TEST_AUDIOS_DIR: {RAW_TEST_AUDIOS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos NORMALIZED\n",
    "NORMALIZED_TRAIN_CSV_PATH = Path(os.path.join(NORMALIZED_DATASET_PATH, 'train_metadata.csv'))\n",
    "print(f\"NORMALIZED_TRAIN_CSV_PATH: {NORMALIZED_TRAIN_CSV_PATH}\")\n",
    "\n",
    "NORMALIZED_TRAIN_AUDIOS_DIR = Path(os.path.join(NORMALIZED_DATASET_PATH, 'train_data'))\n",
    "print(f\"NORMALIZED_TRAIN_AUDIOS_DIR: {NORMALIZED_TRAIN_AUDIOS_DIR}\")\n",
    "\n",
    "NORMALIZED_TEST_CSV_PATH = Path(os.path.join(NORMALIZED_DATASET_PATH, 'test_metadata.csv'))\n",
    "print(f\"NORMALIZED_TEST_CSV_PATH: {NORMALIZED_TEST_CSV_PATH}\")\n",
    "\n",
    "NORMALIZED_TEST_AUDIOS_DIR = Path(os.path.join(NORMALIZED_DATASET_PATH, 'test_data'))\n",
    "print(f\"NORMALIZED_TEST_AUDIOS_DIR: {NORMALIZED_TEST_AUDIOS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df6045",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5822b",
   "metadata": {},
   "source": [
    "#### Normalize test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do CSV\n",
    "df = pd.read_csv(RAW_TEST_CSV_PATH)\n",
    "\n",
    "# Corre√ß√£o e normaliza√ß√£o da coluna de classe\n",
    "df[\"Class\"] = (\n",
    "    df[\"Class\"]\n",
    "    .str.lower()\n",
    "    .str.replace(r\"^sound_\", \"\", regex=True)\n",
    "    .str.replace(r\"(?i)^guiatr$\", \"guitar\", regex=True)\n",
    ")\n",
    "\n",
    "# Guarda o nome original antes da normaliza√ß√£o\n",
    "df[\"OriginalFilename\"] = df[\"FileName\"]\n",
    "\n",
    "# Normaliza o nome dos arquivos\n",
    "df[\"FileName\"] = df[\"FileName\"].apply(normalize_filename)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(NORMALIZED_DATASET_PATH, exist_ok=True)\n",
    "NORMALIZED_TEST_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_test_files = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    original_file = RAW_TEST_AUDIOS_DIR / row[\"OriginalFilename\"]\n",
    "    normalized_file = NORMALIZED_TEST_AUDIOS_DIR / row[\"FileName\"]\n",
    "\n",
    "    if original_file.exists():\n",
    "        shutil.copy2(original_file, normalized_file)\n",
    "    else:\n",
    "        missing_test_files.append(str(original_file))\n",
    "\n",
    "print(f\"missing: {missing_test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"OriginalFilename\"])\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n",
    "    .str.strip(\"_\")\n",
    ")\n",
    "\n",
    "df.to_csv(NORMALIZED_TEST_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f750bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Arquivos normalizados e copiados para: {NORMALIZED_TEST_AUDIOS_DIR}\")\n",
    "print(f\"üìÑ Novo CSV salvo em: {NORMALIZED_TEST_CSV_PATH}\")\n",
    "\n",
    "if missing_test_files:\n",
    "    print(f\"‚ö†Ô∏è Arquivos ausentes ({len(missing_test_files)}):\")\n",
    "    for f in missing_test_files[:5]:\n",
    "        print(f\" - {f}\")\n",
    "else:\n",
    "    print(\"üéâ Todos os arquivos foram encontrados e copiados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e1189",
   "metadata": {},
   "source": [
    "#### Normalize Train Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4352f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do CSV\n",
    "df_train = pd.read_csv(RAW_TRAIN_CSV_PATH)\n",
    "\n",
    "# Corre√ß√£o e normaliza√ß√£o da coluna de classe\n",
    "df_train[\"Class\"] = (\n",
    "    df_train[\"Class\"]\n",
    "    .str.lower()\n",
    "    .str.replace(r\"^sound_\", \"\", regex=True)\n",
    "    .str.replace(r\"(?i)^guiatr$\", \"guitar\", regex=True)\n",
    ")\n",
    "\n",
    "# Guarda o nome original antes da normaliza√ß√£o\n",
    "df_train[\"OriginalFilename\"] = df_train[\"FileName\"]\n",
    "\n",
    "# Normaliza o nome dos arquivos\n",
    "df_train[\"FileName\"] = df_train[\"FileName\"].apply(normalize_filename)\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea598f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(NORMALIZED_DATASET_PATH, exist_ok=True)\n",
    "NORMALIZED_TRAIN_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a09d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_train_files = []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    original_file = RAW_TRAIN_AUDIOS_DIR / row[\"OriginalFilename\"]\n",
    "    normalized_file = NORMALIZED_TRAIN_AUDIOS_DIR / row[\"FileName\"]\n",
    "\n",
    "    if original_file.exists():\n",
    "        shutil.copy2(original_file, normalized_file)\n",
    "    else:\n",
    "        missing_train_files.append(str(original_file))\n",
    "\n",
    "print(f\"missing: {missing_train_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=[\"OriginalFilename\"])\n",
    "df_train.columns = (\n",
    "    df_train.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n",
    "    .str.strip(\"_\")\n",
    ")\n",
    "\n",
    "df_train.to_csv(NORMALIZED_TRAIN_CSV_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Arquivos normalizados e copiados para: {NORMALIZED_TRAIN_AUDIOS_DIR}\")\n",
    "print(f\"üìÑ Novo CSV salvo em: {NORMALIZED_TRAIN_CSV_PATH}\")\n",
    "\n",
    "if missing_train_files:\n",
    "    print(f\"‚ö†Ô∏è Arquivos ausentes ({len(missing_train_files)}):\")\n",
    "    for f in missing_train_files[:5]:\n",
    "        print(f\" - {f}\")\n",
    "else:\n",
    "    print(\"üéâ Todos os arquivos foram encontrados e copiados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12071559",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b40caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "MAX_AUGS_PER_ORIGINAL = 4\n",
    "\n",
    "# Vers√£o do dataset preparado\n",
    "AUGMENTATION_VERSION = \"v1\"\n",
    "\n",
    "# Diret√≥rios base\n",
    "AUGMENTED_DATASET_VERSION_PATH = Path(os.path.join(AUGMENTED_DATASET_PATH, AUGMENTATION_VERSION))\n",
    "AUGMENTED_DATASET_VERSION_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Vers√£o {AUGMENTATION_VERSION}: {AUGMENTED_DATASET_VERSION_PATH}\")\n",
    "\n",
    "AUGMENTED_TRAIN_AUDIOS_DIR = Path(os.path.join(AUGMENTED_DATASET_VERSION_PATH, 'train_data'))\n",
    "AUGMENTED_TRAIN_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AUGMENTED_TRAIN_CSV_PATH = AUGMENTED_DATASET_VERSION_PATH / \"train_metadata.csv\"\n",
    "print(f\"üìÅ Cortes de treino ser√£o salvos em: {AUGMENTED_TRAIN_AUDIOS_DIR}\")\n",
    "print(f\"üìÅ Metadados de treino: {AUGMENTED_TRAIN_CSV_PATH}\")\n",
    "\n",
    "AUGMENTED_TEST_AUDIOS_DIR = Path(os.path.join(AUGMENTED_DATASET_VERSION_PATH, 'test_data'))\n",
    "AUGMENTED_TEST_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AUGMENTED_TEST_CSV_PATH = AUGMENTED_DATASET_VERSION_PATH / \"test_metadata.csv\"\n",
    "print(f\"üìÅ Cortes de treino ser√£o salvos em: {AUGMENTED_TRAIN_AUDIOS_DIR}\")\n",
    "print(f\"üìÅ Metadados de teste: {AUGMENTED_TRAIN_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATION_COMBINATIONS = {\n",
    "    \"original\": [],\n",
    "    \"stretch_102\": [(\"stretch\", 1.02)],\n",
    "    \"pitch_up_0_5\": [(\"pitch\", 0.5)],\n",
    "    \"pitch_down_0_5\": [(\"pitch\", -0.5)],\n",
    "    \"noise_0_003\": [(\"noise\", 0.003)],\n",
    "    \"gain_0_95\": [(\"gain\", 0.95)],\n",
    "    \"gain_1_05\": [(\"gain\", 1.05)],\n",
    "\n",
    "    \"stretch_pitch_up\": [(\"stretch\", 1.02), (\"pitch\", 0.5)],\n",
    "    \"pitch_noise\": [(\"pitch\", -0.5), (\"noise\", 0.002)],\n",
    "    \"stretch_noise\": [(\"stretch\", 0.98), (\"noise\", 0.002)],\n",
    "    \"pitch_gain\": [(\"pitch\", 0.5), (\"gain\", 1.05)],\n",
    "    \"stretch_gain\": [(\"stretch\", 1.02), (\"gain\", 1.05)],\n",
    "}\n",
    "\n",
    "def augment_audio(y, sr, steps):\n",
    "    y_aug = np.copy(y)\n",
    "    for aug_type, param in steps:\n",
    "        if aug_type == \"stretch\":\n",
    "            y_aug = librosa.effects.time_stretch(y_aug, rate=param)\n",
    "        elif aug_type == \"pitch\":\n",
    "            y_aug = librosa.effects.pitch_shift(y_aug, sr=sr, n_steps=param)\n",
    "        elif aug_type == \"noise\":\n",
    "            noise = np.random.normal(0, param, len(y_aug))\n",
    "            y_aug += noise\n",
    "        elif aug_type == \"gain\":\n",
    "            y_aug *= param\n",
    "    return y_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e20d36",
   "metadata": {},
   "source": [
    "#### Augment test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2aa588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(NORMALIZED_TEST_CSV_PATH)\n",
    "df_test.columns = df_test.columns.str.strip().str.lower().str.replace(r\"[^a-z0-9]+\", \"_\", regex=True)\n",
    "\n",
    "augmented_test_records = []\n",
    "grouped_test = df_test.groupby(\"class\")\n",
    "\n",
    "for class_name, group in tqdm(df_test.groupby(\"class\"), desc=\"Test Augmentation\"):\n",
    "    combo_names = list(AUGMENTATION_COMBINATIONS.keys())\n",
    "    combo_names.remove(\"original\")\n",
    "\n",
    "    for i, row in group.reset_index(drop=True).iterrows():\n",
    "        original_path = NORMALIZED_TEST_AUDIOS_DIR / row[\"filename\"]\n",
    "        if not original_path.exists():\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(original_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        original_filename = f\"{original_path.stem}__original.wav\"\n",
    "        output_original_path = AUGMENTED_TEST_AUDIOS_DIR / original_filename\n",
    "        sf.write(output_original_path, y, sr, format=\"WAV\")\n",
    "        augmented_test_records.append({\n",
    "            \"filename\": original_filename,\n",
    "            \"class\": class_name,\n",
    "            \"augmentation\": \"original\"\n",
    "        })\n",
    "\n",
    "        shuffled_combos = random.sample(combo_names, k=len(combo_names))\n",
    "        for j in range(MAX_AUGS_PER_ORIGINAL):\n",
    "            combo = shuffled_combos[j % len(shuffled_combos)]\n",
    "            steps = AUGMENTATION_COMBINATIONS[combo]\n",
    "\n",
    "            y_aug = augment_audio(y, sr, steps)\n",
    "            filename_aug = f\"{original_path.stem}__{combo}.wav\"\n",
    "            output_path = AUGMENTED_TEST_AUDIOS_DIR / filename_aug\n",
    "\n",
    "            sf.write(output_path, y_aug, sr, format=\"WAV\")\n",
    "            augmented_test_records.append({\n",
    "                \"filename\": filename_aug,\n",
    "                \"class\": class_name,\n",
    "                \"augmentation\": combo\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_test = pd.DataFrame(augmented_test_records)\n",
    "df_augmented_test.to_csv(AUGMENTED_TEST_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"üìÑ CSV de metadados de teste salvo em: {AUGMENTED_TEST_CSV_PATH}\")\n",
    "print(f\"üéß Total de arquivos augmentados no teste: {len(df_augmented_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8668724",
   "metadata": {},
   "source": [
    "#### Augment train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65306a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv(NORMALIZED_TRAIN_CSV_PATH)\n",
    "grouped = df_train.groupby(\"class\")\n",
    "\n",
    "augmented_train_records = []\n",
    "\n",
    "\n",
    "for class_name, group in tqdm(df_train.groupby(\"class\"), desc=\"Train Augmentation\"):\n",
    "    combo_names = list(AUGMENTATION_COMBINATIONS.keys())\n",
    "    combo_names.remove(\"original\")\n",
    "\n",
    "    for i, row in group.reset_index(drop=True).iterrows():\n",
    "        original_path = NORMALIZED_TRAIN_AUDIOS_DIR / row[\"filename\"]\n",
    "        if not original_path.exists():\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(original_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        original_filename = f\"{original_path.stem}__original.wav\"\n",
    "        output_original_path = AUGMENTED_TRAIN_AUDIOS_DIR / original_filename\n",
    "        sf.write(output_original_path, y, sr, format=\"WAV\")\n",
    "        augmented_train_records.append({\n",
    "            \"filename\": original_filename,\n",
    "            \"class\": class_name,\n",
    "            \"augmentation\": \"original\"\n",
    "        })\n",
    "\n",
    "        shuffled_combos = random.sample(combo_names, k=len(combo_names))\n",
    "        for j in range(MAX_AUGS_PER_ORIGINAL):\n",
    "            combo = shuffled_combos[j % len(shuffled_combos)]\n",
    "            steps = AUGMENTATION_COMBINATIONS[combo]\n",
    "\n",
    "            y_aug = augment_audio(y, sr, steps)\n",
    "            filename_aug = f\"{original_path.stem}__{combo}.wav\"\n",
    "            output_path = AUGMENTED_TRAIN_AUDIOS_DIR / filename_aug\n",
    "\n",
    "            sf.write(output_path, y_aug, sr, format=\"WAV\")\n",
    "            augmented_train_records.append({\n",
    "                \"filename\": filename_aug,\n",
    "                \"class\": class_name,\n",
    "                \"augmentation\": combo\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1883cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_train = pd.DataFrame(augmented_train_records)\n",
    "df_augmented_train.to_csv(AUGMENTED_TRAIN_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"üìÑ CSV de metadados salvo em: {AUGMENTED_TRAIN_CSV_PATH}\")\n",
    "print(f\"üéß Total de arquivos augmentados: {len(df_augmented_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255cc6d7",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "WINDOW_SIZE = 3.0    # segundos\n",
    "OVERLAP = 1.5        # segundos\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "PREPARATION_VERSION = \"v1\"\n",
    "AUGMENTATION_VERSION_SOURCE = \"v1\"\n",
    "\n",
    "# Diret√≥rios base\n",
    "PREPARED_DATASET_VERSION_PATH = Path(os.path.join(PREPARED_DATASET_PATH, PREPARATION_VERSION))\n",
    "PREPARED_DATASET_VERSION_PATH.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Vers√£o {PREPARATION_VERSION}: {PREPARED_DATASET_VERSION_PATH}\")\n",
    "\n",
    "PREPARED_TRAIN_AUDIOS_DIR = Path(os.path.join(PREPARED_DATASET_VERSION_PATH, 'train_data'))\n",
    "PREPARED_TRAIN_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Cortes de treino ser√£o salvos em: {PREPARED_TRAIN_AUDIOS_DIR}\")\n",
    "\n",
    "PREPARED_TEST_AUDIOS_DIR = Path(os.path.join(PREPARED_DATASET_VERSION_PATH, 'test_data'))\n",
    "PREPARED_TEST_AUDIOS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Cortes de teste ser√£o salvos em: {PREPARED_TRAIN_AUDIOS_DIR}\")\n",
    "\n",
    "AUGMENTED_PREPARE_DATASET_VERSION_PATH = Path(os.path.join(AUGMENTED_DATASET_PATH, AUGMENTATION_VERSION_SOURCE))\n",
    "\n",
    "print(f\"üìÅ Fonte dataset augmented: {AUGMENTED_PREPARE_DATASET_VERSION_PATH}\")\n",
    "\n",
    "AUGMENTED_PREPARE_TEST_AUDIOS_DIR = Path(os.path.join(AUGMENTED_DATASET_VERSION_PATH, 'test_data'))\n",
    "AUGMENTED_PREPARE_TEST_CSV_PATH = AUGMENTED_PREPARE_DATASET_VERSION_PATH / \"test_metadata.csv\"\n",
    "\n",
    "AUGMENTED_PREPARE_TRAIN_AUDIOS_DIR = Path(os.path.join(AUGMENTED_DATASET_VERSION_PATH, 'train_data'))\n",
    "AUGMENTED_PREPARE_TRAIN_CSV_PATH = AUGMENTED_PREPARE_DATASET_VERSION_PATH / \"train_metadata.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_audio_with_overlap(file_path: Path, class_name: str, output_dir: Path, \n",
    "                             window_size: float, overlap: float, sample_rate: int = 22050):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar {file_path.name}: {e}\")\n",
    "        return []\n",
    "\n",
    "    window_samples = int(window_size * sr)\n",
    "    hop_samples = int((window_size - overlap) * sr)\n",
    "\n",
    "    segments = []\n",
    "    base_name = file_path.stem  # Nome base sem extens√£o\n",
    "\n",
    "    for i, start in enumerate(range(0, len(y), hop_samples)):\n",
    "        end = start + window_samples\n",
    "        slice_audio = y[start:end]\n",
    "\n",
    "        # Padding com zeros se for menor que o necess√°rio\n",
    "        if len(slice_audio) < window_samples:\n",
    "            slice_audio = np.pad(slice_audio, (0, window_samples - len(slice_audio)))\n",
    "\n",
    "        # Gera nome padronizado com extens√£o .wav\n",
    "        filename_seq = f\"{base_name}_{i+1:06}.wav\"\n",
    "        out_path = output_dir / filename_seq\n",
    "\n",
    "        try:\n",
    "            sf.write(out_path, slice_audio, sr, format='WAV')\n",
    "            segments.append((filename_seq, class_name))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao salvar {out_path.name}: {e}\")\n",
    "\n",
    "        if end >= len(y):\n",
    "            break\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6e19d",
   "metadata": {},
   "source": [
    "#### Prepare test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(AUGMENTED_PREPARE_TEST_CSV_PATH)\n",
    "\n",
    "prepared_test_data = []\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    original_path = AUGMENTED_PREPARE_TEST_AUDIOS_DIR / row[\"filename\"]\n",
    "    class_name = row[\"class\"]\n",
    "\n",
    "    if original_path.exists():\n",
    "        segments = slice_audio_with_overlap(\n",
    "            file_path=original_path,\n",
    "            class_name=class_name,\n",
    "            output_dir=PREPARED_TEST_AUDIOS_DIR,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            overlap=OVERLAP,\n",
    "            sample_rate=SAMPLE_RATE\n",
    "        )\n",
    "        prepared_test_data.extend(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008508c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared_test = pd.DataFrame(prepared_test_data, columns=[\"filename\", \"class\"])\n",
    "\n",
    "PREPARED_TEST_CSV_PATH = PREPARED_DATASET_VERSION_PATH / \"test_metadata.csv\"\n",
    "df_prepared_test.to_csv(PREPARED_TEST_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"üìÑ Metadata de teste salva em: {PREPARED_TEST_CSV_PATH}\")\n",
    "print(f\"üéß Total de segmentos: {len(df_prepared_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2b1eb",
   "metadata": {},
   "source": [
    "#### Prepare train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(AUGMENTED_PREPARE_TRAIN_CSV_PATH)\n",
    "\n",
    "prepared_train_data = []\n",
    "\n",
    "for _, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    original_path = AUGMENTED_PREPARE_TRAIN_AUDIOS_DIR / row[\"filename\"]\n",
    "    class_name = row[\"class\"]\n",
    "\n",
    "    if original_path.exists():\n",
    "        segments = slice_audio_with_overlap(\n",
    "            file_path=original_path,\n",
    "            class_name=class_name,\n",
    "            output_dir=PREPARED_TRAIN_AUDIOS_DIR,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            overlap=OVERLAP,\n",
    "            sample_rate=SAMPLE_RATE\n",
    "        )\n",
    "        prepared_train_data.extend(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared_train = pd.DataFrame(prepared_train_data, columns=[\"filename\", \"class\"])\n",
    "\n",
    "PREPARED_TRAIN_CSV_PATH = PREPARED_DATASET_VERSION_PATH / \"train_prepared_metadata.csv\"\n",
    "df_prepared_train.to_csv(PREPARED_TRAIN_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"üìÑ Metadata de treino salva em: {PREPARED_TRAIN_CSV_PATH}\")\n",
    "print(f\"üéß Total de segmentos: {len(df_prepared_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c516c",
   "metadata": {},
   "source": [
    "#### Verify Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686be5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHECK_TEST_DIR = PREPARED_DATASET_VERSION_PATH / \"test_data\"\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "sample_test_lengths = []\n",
    "\n",
    "for path in tqdm(sorted(CHECK_TEST_DIR.glob(\"*.wav\"))):\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "        sample_test_lengths.append(len(y))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {path.name}: {e}\")\n",
    "\n",
    "length_test_counts = Counter(sample_test_lengths)\n",
    "\n",
    "print(\"üìä Distribui√ß√£o dos tamanhos encontrados (em n√∫mero de samples):\")\n",
    "for length, count in length_test_counts.items():\n",
    "    duration = length / SAMPLE_RATE\n",
    "    print(f\"- {length} samples (~{duration:.2f}s): {count} arquivos\")\n",
    "\n",
    "if len(length_test_counts) == 1:\n",
    "    print(\"‚úÖ Todos os arquivos t√™m o mesmo tamanho!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Existem arquivos com tamanhos diferentes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHECK_TRAIN_DIR = PREPARED_DATASET_VERSION_PATH / \"train_data\"\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "sample_train_lengths = []\n",
    "\n",
    "for path in tqdm(sorted(CHECK_TRAIN_DIR.glob(\"*.wav\"))):\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "        sample_train_lengths.append(len(y))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {path.name}: {e}\")\n",
    "\n",
    "length_train_counts = Counter(sample_train_lengths)\n",
    "\n",
    "print(\"üìä Distribui√ß√£o dos tamanhos encontrados (em n√∫mero de samples):\")\n",
    "for length, count in length_train_counts.items():\n",
    "    duration = length / SAMPLE_RATE\n",
    "    print(f\"- {length} samples (~{duration:.2f}s): {count} arquivos\")\n",
    "\n",
    "if len(length_train_counts) == 1:\n",
    "    print(\"‚úÖ Todos os arquivos t√™m o mesmo tamanho!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Existem arquivos com tamanhos diferentes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
