{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b87a57",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c45a3b",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ['ABSL_LOG_THRESHOLD'] = '0'\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"Is TensorFlow using GPU?\", tf.test.is_gpu_available())\n",
    "print(\"GPU disponível:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"XLA ativado:\", tf.config.optimizer.get_jit())\n",
    "# Mostra configuração geral\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_PATH = '../datasets/training'\n",
    "TRAINING_DATASET_VERSION = 'v2'\n",
    "\n",
    "TRAINING_DATASET_VERSION_PATH = Path(os.path.join(TRAINING_DATASET_PATH, TRAINING_DATASET_VERSION))\n",
    "\n",
    "TRAINING_DATASET_TRAIN_METADATA = TRAINING_DATASET_VERSION_PATH / 'train_metadata.csv'\n",
    "TRAINING_DATASET_TRAIN_DATA = TRAINING_DATASET_VERSION_PATH / 'train_data'\n",
    "\n",
    "TRAINING_DATASET_TEST_METADATA = TRAINING_DATASET_VERSION_PATH / 'test_metadata.csv'\n",
    "TRAINING_DATASET_TEST_DATA = TRAINING_DATASET_VERSION_PATH / 'test_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267558f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos metadados\n",
    "train_meta = pd.read_csv(TRAINING_DATASET_TRAIN_METADATA)#.drop(columns=[\"augmentation\"])\n",
    "test_meta = pd.read_csv(TRAINING_DATASET_TEST_METADATA)#.drop(columns=[\"augmentation\"])\n",
    "\n",
    "# Validação de classes únicas\n",
    "train_classes = set(train_meta['class'].unique())\n",
    "test_classes = set(test_meta['class'].unique())\n",
    "\n",
    "if train_classes != test_classes:\n",
    "    raise ValueError(f\"As classes do teste não batem com as do treino.\\nTreino: {train_classes}\\nTeste: {test_classes}\")\n",
    "\n",
    "# Mapeamento único\n",
    "unique_classes = sorted(train_classes)\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Análise das distribuições\n",
    "print(\"📊 Distribuição no treino:\")\n",
    "print(train_meta['class'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n📊 Distribuição no teste:\")\n",
    "print(test_meta['class'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n🔢 Mapeamento de classes:\")\n",
    "for cls, idx in class_to_idx.items():\n",
    "    print(f\"{cls}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53471544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo automático dos pesos\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(list(class_to_idx.values())),\n",
    "    y=train_meta['class'].map(class_to_idx).values\n",
    ")\n",
    "\n",
    "# Transforma em dicionário\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(\"class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5aab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_npz_generator(meta_df, data_dir, class_map, augment=False):\n",
    "    for _, row in meta_df.iterrows():\n",
    "        path = data_dir / row['filename']\n",
    "        try:\n",
    "            mel = np.load(path)['mel'].astype(np.float32)\n",
    "            mel = mel[:, :128]            \n",
    "            label = class_map[row['class']]\n",
    "            yield mel[..., np.newaxis], label\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "input_shape = (128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6608397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsets (definidos como antes)\n",
    "TRAIN_SAMPLE_SIZE = 1400\n",
    "VAL_SAMPLE_SIZE = 600\n",
    "\n",
    "# TRAIN_SAMPLE_SIZE = 140000\n",
    "# VAL_SAMPLE_SIZE = 6000\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "train_meta_sampled = train_meta#.sample(n=TRAIN_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "val_meta_sampled = test_meta#.sample(n=VAL_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Class weights baseados no subset\n",
    "subset_class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(list(class_to_idx.values())),\n",
    "    y=train_meta_sampled['class'].map(class_to_idx).values\n",
    ")\n",
    "subset_class_weight_dict = {i: w for i, w in enumerate(subset_class_weights)}\n",
    "\n",
    "# Dataset de treino\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: audio_npz_generator(train_meta_sampled, TRAINING_DATASET_TRAIN_DATA, class_to_idx, augment=True),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Dataset de validação\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: audio_npz_generator(val_meta_sampled, TRAINING_DATASET_TEST_DATA, class_to_idx),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔢 Quantidade alvo por classe\n",
    "SAMPLES_PER_CLASS = 1000  # ajuste conforme necessário\n",
    "\n",
    "# 📊 Balanceamento: faz oversampling se precisar\n",
    "balanced_train_meta = pd.concat([\n",
    "    resample(\n",
    "        group,\n",
    "        replace=True,\n",
    "        n_samples=SAMPLES_PER_CLASS,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    for _, group in train_meta_sampled.groupby('class')\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# 🧾 Verificação da distribuição\n",
    "print(\"📊 Distribuição balanceada:\")\n",
    "print(balanced_train_meta['class'].value_counts().sort_index())\n",
    "\n",
    "balanced_train_meta.head()\n",
    "\n",
    "balanced_train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: audio_npz_generator(balanced_train_meta, TRAINING_DATASET_TRAIN_DATA, class_to_idx, augment=True),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=input_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf81c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_to_idx)\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        # tfa.metrics.F1Score(num_classes=4, average='macro', name='f1_macro'),\n",
    "        # tfa.metrics.Precision(name='precision'),\n",
    "        # tfa.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['ABSL_LOG_THRESHOLD'] = '3'\n",
    "\n",
    "EPOCHS = 2\n",
    "dataset = train_ds\n",
    "metadata = train_meta \n",
    "\n",
    "steps_per_epoch = math.ceil(len(metadata) / BATCH_SIZE)\n",
    "\n",
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(list(class_to_idx.values())),\n",
    "    y=metadata['class'].map(class_to_idx).values\n",
    ")\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    class_weight=subset_class_weight_dict\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22b58b",
   "metadata": {},
   "source": [
    "## Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_dict = history.history\n",
    "\n",
    "# Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Treino')\n",
    "plt.plot(history_dict['val_loss'], label='Validação')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict['accuracy'], label='Treino')\n",
    "plt.plot(history_dict['val_accuracy'], label='Validação')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29041f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch_x, batch_y in val_ds:\n",
    "    preds = model.predict(batch_x, verbose=0)\n",
    "    y_true.extend(batch_y.numpy())\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# Relatório de classificação\n",
    "print(classification_report(y_true, y_pred, target_names=class_to_idx.keys()))\n",
    "\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('confusion')\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(class_to_idx.keys()))\n",
    "disp.plot(xticks_rotation=45, cmap='Blues')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9431f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
